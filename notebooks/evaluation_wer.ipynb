{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaqIXb4qZ9BD"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hlw6LApnXRf"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBe32tqOaAs2"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_PsbOjx_6St"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModelForSeq2SeqLM, BartTokenizer, BartForConditionalGeneration\n",
        "import datasets\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SehC_7WU8tdX"
      },
      "outputs": [],
      "source": [
        "def error_correct(text, model_name):\n",
        "\n",
        "\n",
        "  # model = AutoModelForSeq2SeqLM.from_pretrained(model_name) #T5\n",
        "  # tokenizer = AutoTokenizer.from_pretrained(model_name) #T5\n",
        "\n",
        "  model = BartForConditionalGeneration.from_pretrained(model_name) #BART\n",
        "  tokenizer = BartTokenizer.from_pretrained(model_name) #BART\n",
        "\n",
        "\n",
        "  input_ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "  generated_ids = model.generate(input_ids=input_ids, \n",
        "                                 num_return_sequences=5, \n",
        "                                 num_beams=5, \n",
        "                                 max_length=512, \n",
        "                                 no_repeat_ngram_size=2, \n",
        "                                 repetition_penalty=3.5, \n",
        "                                 length_penalty=1.0, \n",
        "                                 early_stopping=True\n",
        "                                 )\n",
        "\n",
        "  preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "\n",
        "  return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IUUsvP7aE7m"
      },
      "source": [
        "## Trained Models\n",
        "#### Please note that all the trained model are currently available in Huggingface_hub and due to the anonimity requirement we will share the trained models in the following Google Drive link: https://drive.google.com/drive/folders/1uowiKAgk3DW48QumeCXEoDzuijN8iqTV?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQwRXgzX_MgF"
      },
      "outputs": [],
      "source": [
        "#base models - from literature\n",
        "bart_base = 'facebook/bart-base'\n",
        "bart_large = 'facebook/bart-large'\n",
        "\n",
        "'''BART fine-tuned models'''\n",
        "\n",
        "# Standard Objective\n",
        "bart_clinical = '../models/bart-finetuned-pubmed'\n",
        "bart_pubmed = '../models/bart-paraphrase-pubmed-1.1'\n",
        "bart_mlm = '../models/bart-mlm-pubmed'\n",
        "\n",
        "# Hybrid Objective\n",
        "bart_mlm_paraphrasing = '../models/bart-mlm-paraphrasing'\n",
        "bart_paraphrasing_mlm = '../models/bart-paraphrasing-mlm'\n",
        "\n",
        "# Domain-specific Objective\n",
        "bart_med_term = '../models/bart-mlm-pubmed-medterm'\n",
        "bart_cm = '../models/bart-med-term-conditional-masking'\n",
        "bart_cm_0 = '../models/bart-med-term-conditional-masking-0'\n",
        "\n",
        "\n",
        "'''T5 fine-tuned models'''\n",
        "\n",
        "# Standard Objective\n",
        "t5_clinical = '../models/t5-small-finetuned-pubmed'\n",
        "t5_pubmed = '../models/t5-small-paraphrase-pubmed'\n",
        "t5_mlm = '../models/t5-small-mlm-pubmed'\n",
        "\n",
        "# Hybrid Objective\n",
        "t5_mlm_paraphrasing = '../models/t5-small-mlm-paraphrasing'\n",
        "t5_small_paraphrasing_mlm = '../models/t5-small-paraphrasing-mlm'\n",
        "\n",
        "# Domain-specific Objective\n",
        "t5_small_med_term_mlm = '../models/t5-small-med-term-mlm'\n",
        "t5_small_cm = '../models/t5-small-med-term-conditional-masking'\n",
        "t5_small_cm_0 = '../models/t5-small-med-term-conditional-masking-0'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx2ahu77Witv"
      },
      "source": [
        "## Evaluation - WER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R11clzhGlqbZ"
      },
      "outputs": [],
      "source": [
        "metric = datasets.load_metric('wer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Data\n",
        "\n",
        "#### This clinical ocnversational dataset is collected in colaboration with NHS Grampian. Therefore the dataset will be available upon request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21QkW9Q1soe3"
      },
      "outputs": [],
      "source": [
        "test_data_aws =  '../datasets/test/refs_and_trans_aws_gb.csv'\n",
        "test_data_ms =  '../datasets/refs_and_trans_ms_gb.csv'\n",
        "test_data_ibm =  '../datasets/refs_and_trans_ibm_gb.csv'\n",
        "test_data_google =  '../datasets/refs_and_trans_google_gb.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ4EWyIYW0GH"
      },
      "outputs": [],
      "source": [
        "def calculate_wer(test_data, pre_trained_model):\n",
        "\n",
        "  test_df = pd.read_csv(test_data)\n",
        "\n",
        "  test_df = test_df.dropna()\n",
        "\n",
        "  test_df['modified'] = test_df.apply (lambda row: error_correct(row.trans, pre_trained_model), axis=1)\n",
        "  out = []\n",
        "  for n, row in test_df.iterrows():\n",
        "    for item in row['modified']:\n",
        "      row['flat_modified'] = item\n",
        "      out += [row.copy()]\n",
        "\n",
        "\n",
        "  flattened_df = pd.DataFrame(out)\n",
        "\n",
        "  trans_batch = flattened_df.flat_modified.tolist()\n",
        "  reference_batch = flattened_df.refs.tolist()\n",
        "\n",
        "  score = metric.compute(predictions=trans_batch, references=reference_batch)\n",
        "\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjdi4LwrEHmH"
      },
      "outputs": [],
      "source": [
        "score_aws = calculate_wer(test_data_aws, bart_clinical)\n",
        "score_ms = calculate_wer(test_data_ms, bart_clinical)\n",
        "score_ibm = calculate_wer(test_data_ibm, bart_clinical)\n",
        "score_google = calculate_wer(test_data_google, bart_clinical)\n",
        "\n",
        "print('score_aws: ', score_aws)\n",
        "print('score_ms: ', score_ms)\n",
        "print('score_ibm: ', score_ibm)\n",
        "print('score_google: ', score_google)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "aacl-ijcnlp-evaluation-wer.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
